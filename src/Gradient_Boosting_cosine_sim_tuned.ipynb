{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient_Boosting_cosine_sim_tuned.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIBdI8ZdCLtZ",
        "colab_type": "text"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5xUfYD7C9mE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "3ddcdab0-f267-44f7-f065-7aa452243e95"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKDziZErvCNI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "cb15418b-dd97-4ecf-b211-979f098767d4"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "_wnl = nltk.WordNetLemmatizer()\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "from csv import DictReader\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "# sklearn dependencies\n",
        "from sklearn import feature_extraction\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Function for reading the dataset\n",
        "class DataSet():\n",
        "    def __init__(self, name=\"train\", path=\"/content/drive/My Drive/fnc-1\"):\n",
        "        self.path = path\n",
        "\n",
        "        print(\"Reading dataset\")\n",
        "        bodies = name+\"_bodies.csv\"\n",
        "        stances = name+\"_stances.csv\"\n",
        "        \n",
        "        self.stances = self.read(stances)\n",
        "       \n",
        "        articles = self.read(bodies)\n",
        "       \n",
        "        self.articles = dict()\n",
        "\n",
        "        #make the body ID an integer value\n",
        "        for s in self.stances:\n",
        "            s['Body ID'] = int(s['Body ID'])\n",
        "        \n",
        "        #copy all bodies into a dictionary\n",
        "        for article in articles:\n",
        "            self.articles[int(article['Body ID'])] = article['articleBody']\n",
        "\n",
        "        print(\"Total stances: \" + str(len(self.stances)))\n",
        "        print(\"Total bodies: \" + str(len(self.articles)))\n",
        "\n",
        "    \n",
        "\n",
        "    def read(self,filename):\n",
        "        rows = []\n",
        "        with open(self.path + \"/\" + filename, \"r\", encoding='utf-8') as table:\n",
        "            r = DictReader(table)\n",
        "\n",
        "            for line in r:\n",
        "                rows.append(line)\n",
        "        return rows\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRmFdmwUyio9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SimEgIk-C1H1",
        "colab_type": "text"
      },
      "source": [
        "### Functions to form various features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXG88TCotLR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature Functions(taken from baseline and added cosine similarity)\n",
        "def normalize_word(w):\n",
        "    return _wnl.lemmatize(w).lower()\n",
        "\n",
        "\n",
        "def get_tokenized_lemmas(s):\n",
        "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
        "\n",
        "\n",
        "def clean(s):\n",
        "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
        "\n",
        "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
        "\n",
        "\n",
        "def remove_stopwords(l):\n",
        "    # Removes stopwords from a list of tokens\n",
        "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
        "\n",
        "\n",
        "def gen_or_load_feats(feat_fn, headlines, bodies, feature_file):\n",
        "  # if not os.path.isfile(feature_file):\n",
        "    feats = feat_fn(headlines, bodies)\n",
        "    np.save(feature_file, feats)\n",
        "    return np.load(feature_file)\n",
        "\n",
        "def word_overlap_features(headlines, bodies):\n",
        "    X = []\n",
        "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
        "        clean_headline = clean(headline)\n",
        "        clean_body = clean(body)\n",
        "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
        "        clean_body = get_tokenized_lemmas(clean_body)\n",
        "        features = [\n",
        "            len(set(clean_headline).intersection(clean_body)) / float(len(set(clean_headline).union(clean_body)))]\n",
        "        X.append(features)\n",
        "    return X\n",
        "\n",
        "\n",
        "def refuting_features(headlines, bodies):\n",
        "    _refuting_words = [\n",
        "        'fake',\n",
        "        'fraud',\n",
        "        'hoax',\n",
        "        'false',\n",
        "        'deny', 'denies',\n",
        "        # 'refute',\n",
        "        'not',\n",
        "        'despite',\n",
        "        'nope',\n",
        "        'doubt', 'doubts',\n",
        "        'bogus',\n",
        "        'debunk',\n",
        "        'pranks',\n",
        "        'retract'\n",
        "    ]\n",
        "    X = []\n",
        "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
        "        clean_headline = clean(headline)\n",
        "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
        "        features = [1 if word in clean_headline else 0 for word in _refuting_words]\n",
        "        X.append(features)\n",
        "    return X\n",
        "\n",
        "\n",
        "def polarity_features(headlines, bodies):\n",
        "    _refuting_words = [\n",
        "        'fake',\n",
        "        'fraud',\n",
        "        'hoax',\n",
        "        'false',\n",
        "        'deny', 'denies',\n",
        "        'not',\n",
        "        'despite',\n",
        "        'nope',\n",
        "        'doubt', 'doubts',\n",
        "        'bogus',\n",
        "        'debunk',\n",
        "        'pranks',\n",
        "        'retract'\n",
        "    ]\n",
        "\n",
        "    def calculate_polarity(text):\n",
        "        tokens = get_tokenized_lemmas(text)\n",
        "        return sum([t in _refuting_words for t in tokens]) % 2\n",
        "    X = []\n",
        "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
        "        clean_headline = clean(headline)\n",
        "        clean_body = clean(body)\n",
        "        features = []\n",
        "        features.append(calculate_polarity(clean_headline))\n",
        "        features.append(calculate_polarity(clean_body))\n",
        "        X.append(features)\n",
        "    return np.array(X)\n",
        "\n",
        "\n",
        "def ngrams(input, n):\n",
        "    input = input.split(' ')\n",
        "    output = []\n",
        "    for i in range(len(input) - n + 1):\n",
        "        output.append(input[i:i + n])\n",
        "    return output\n",
        "\n",
        "\n",
        "def chargrams(input, n):\n",
        "    output = []\n",
        "    for i in range(len(input) - n + 1):\n",
        "        output.append(input[i:i + n])\n",
        "    return output\n",
        "\n",
        "\n",
        "def append_chargrams(features, text_headline, text_body, size):\n",
        "    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n",
        "    grams_hits = 0\n",
        "    grams_early_hits = 0\n",
        "    grams_first_hits = 0\n",
        "    for gram in grams:\n",
        "        if gram in text_body:\n",
        "            grams_hits += 1\n",
        "        if gram in text_body[:255]:\n",
        "            grams_early_hits += 1\n",
        "        if gram in text_body[:100]:\n",
        "            grams_first_hits += 1\n",
        "    features.append(grams_hits)\n",
        "    features.append(grams_early_hits)\n",
        "    features.append(grams_first_hits)\n",
        "    return features\n",
        "\n",
        "\n",
        "def append_ngrams(features, text_headline, text_body, size):\n",
        "    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n",
        "    grams_hits = 0\n",
        "    grams_early_hits = 0\n",
        "    for gram in grams:\n",
        "        if gram in text_body:\n",
        "            grams_hits += 1\n",
        "        if gram in text_body[:255]:\n",
        "            grams_early_hits += 1\n",
        "    features.append(grams_hits)\n",
        "    features.append(grams_early_hits)\n",
        "    return features\n",
        "\n",
        "def cosine_similarity_h(headlines, bodies):\n",
        "\tvectorizer = TfidfVectorizer(ngram_range=(1,2), lowercase=True, stop_words='english')#, max_features=1024)\n",
        "\n",
        "\tcos_sim_features = []\n",
        "\tfor i in range(0, len(bodies)):\n",
        "\t\tbody_headline = []\n",
        "\t\tbody_headline.append(bodies[i])\n",
        "\t\tbody_headline.append(headlines[i])\n",
        "\t\ttfidf = vectorizer.fit_transform(body_headline)\n",
        "\n",
        "\t\tcosine_similarity = (tfidf * tfidf.T).A\n",
        "\t\tcos_sim_features.append(cosine_similarity[0][1])\n",
        "\n",
        "\t\n",
        "\tcos_sim_array = np.array(cos_sim_features) \n",
        "\n",
        "\treturn cos_sim_array\n",
        "\n",
        "def hand_features(headlines, bodies):\n",
        "\n",
        "    def binary_co_occurence(headline, body):\n",
        "        # Count how many times a token in the title\n",
        "        # appears in the body text.\n",
        "        bin_count = 0\n",
        "        bin_count_early = 0\n",
        "        for headline_token in clean(headline).split(\" \"):\n",
        "            if headline_token in clean(body):\n",
        "                bin_count += 1\n",
        "            if headline_token in clean(body)[:255]:\n",
        "                bin_count_early += 1\n",
        "        return [bin_count, bin_count_early]\n",
        "\n",
        "    def binary_co_occurence_stops(headline, body):\n",
        "        # Count how many times a token in the title\n",
        "        # appears in the body text. Stopwords in the title\n",
        "        # are ignored.\n",
        "        bin_count = 0\n",
        "        bin_count_early = 0\n",
        "        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n",
        "            if headline_token in clean(body):\n",
        "                bin_count += 1\n",
        "                bin_count_early += 1\n",
        "        return [bin_count, bin_count_early]\n",
        "\n",
        "    def count_grams(headline, body):\n",
        "        # Count how many times an n-gram of the title\n",
        "        # appears in the entire body, and intro paragraph\n",
        "\n",
        "        clean_body = clean(body)\n",
        "        clean_headline = clean(headline)\n",
        "        features = []\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 2)\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 8)\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 4)\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 16)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 2)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 3)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 4)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 5)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 6)\n",
        "        return features\n",
        "\n",
        "    X = []\n",
        "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
        "        X.append(binary_co_occurence(headline, body)\n",
        "                 + binary_co_occurence_stops(headline, body)\n",
        "                 + count_grams(headline, body))\n",
        "\n",
        "\n",
        "    return X\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntlcxEiDDLsv",
        "colab_type": "text"
      },
      "source": [
        "### Functions to generate splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRuNhRARuq3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Functions to generate splits\n",
        "\n",
        "def generate_hold_out_split (dataset, training = 0.8, base_dir=\"/content/drive/My Drive/fnc-1\"):\n",
        "    r = random.Random()\n",
        "    r.seed(1489215)\n",
        "\n",
        "    article_ids = list(dataset.articles.keys())  # get a list of article ids\n",
        "    r.shuffle(article_ids)  # and shuffle that list\n",
        "\n",
        "\n",
        "    training_ids = article_ids[:int(training * len(article_ids))]\n",
        "    hold_out_ids = article_ids[int(training * len(article_ids)):]\n",
        "\n",
        "    # write the split body ids out to files for future use\n",
        "    with open(base_dir+ \"/\"+ \"training_ids.txt\", \"w+\") as f:\n",
        "        f.write(\"\\n\".join([str(id) for id in training_ids]))\n",
        "\n",
        "    with open(base_dir+ \"/\"+ \"hold_out_ids.txt\", \"w+\") as f:\n",
        "        f.write(\"\\n\".join([str(id) for id in hold_out_ids]))\n",
        "\n",
        "\n",
        "\n",
        "def read_ids(file,base):\n",
        "    ids = []\n",
        "    with open(base+\"/\"+file,\"r\") as f:\n",
        "        for line in f:\n",
        "           ids.append(int(line))\n",
        "        return ids\n",
        "\n",
        "\n",
        "def kfold_split(dataset, training = 0.8, n_folds = 10, base_dir=\"/content/drive/My Drive/fnc-1\"):\n",
        "    if not (os.path.exists(base_dir+ \"/\"+ \"training_ids.txt\")\n",
        "            and os.path.exists(base_dir+ \"/\"+ \"hold_out_ids.txt\")):\n",
        "        generate_hold_out_split(dataset,training,base_dir)\n",
        "\n",
        "    training_ids = read_ids(\"training_ids.txt\", base_dir)\n",
        "    hold_out_ids = read_ids(\"hold_out_ids.txt\", base_dir)\n",
        "\n",
        "    folds = []\n",
        "    for k in range(n_folds):\n",
        "        folds.append(training_ids[int(k*len(training_ids)/n_folds):int((k+1)*len(training_ids)/n_folds)])\n",
        "\n",
        "    return folds,hold_out_ids\n",
        "\n",
        "\n",
        "def get_stances_for_folds(dataset,folds,hold_out):\n",
        "    stances_folds = defaultdict(list)\n",
        "    stances_hold_out = []\n",
        "    for stance in dataset.stances:\n",
        "        if stance['Body ID'] in hold_out:\n",
        "            stances_hold_out.append(stance)\n",
        "        else:\n",
        "            fold_id = 0\n",
        "            for fold in folds:\n",
        "                if stance['Body ID'] in fold:\n",
        "                    stances_folds[fold_id].append(stance)\n",
        "                fold_id += 1\n",
        "\n",
        "    return stances_folds,stances_hold_out\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1-wXahADS4t",
        "colab_type": "text"
      },
      "source": [
        "### Functions to generate the score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctcVfMkpvN7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Score functions\n",
        "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "LABELS_RELATED = ['unrelated','related']\n",
        "RELATED = LABELS[0:3]\n",
        "\n",
        "def score_submission(gold_labels, test_labels):\n",
        "    score = 0.0\n",
        "    cm = [[0, 0, 0, 0],\n",
        "          [0, 0, 0, 0],\n",
        "          [0, 0, 0, 0],\n",
        "          [0, 0, 0, 0]]\n",
        "\n",
        "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
        "        g_stance, t_stance = g, t\n",
        "        if g_stance == t_stance:\n",
        "            score += 0.25\n",
        "            if g_stance != 'unrelated':\n",
        "                score += 0.50\n",
        "        if g_stance in RELATED and t_stance in RELATED:\n",
        "            score += 0.25\n",
        "\n",
        "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
        "\n",
        "    return score, cm\n",
        "\n",
        "# function for printing confusion Matrix\n",
        "def print_confusion_matrix(cm):\n",
        "    lines = []\n",
        "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
        "    line_len = len(header)\n",
        "    lines.append(\"-\"*line_len)\n",
        "    lines.append(header)\n",
        "    lines.append(\"-\"*line_len)\n",
        "\n",
        "    hit = 0\n",
        "    total = 0\n",
        "    for i, row in enumerate(cm):\n",
        "        hit += row[i]\n",
        "        total += sum(row)\n",
        "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
        "                                                                   *row))\n",
        "        lines.append(\"-\"*line_len)\n",
        "    print('\\n'.join(lines))\n",
        "\n",
        "\n",
        "def report_score(actual,predicted):\n",
        "    score,cm = score_submission(actual,predicted)\n",
        "    best_score, _ = score_submission(actual,actual)\n",
        "\n",
        "    print_confusion_matrix(cm)\n",
        "    print(\"Score: \" +str(score) + \" out of \" + str(best_score) + \"\\t(\"+str(score*100/best_score) + \"%)\")\n",
        "    return score*100/best_score\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTtqx_CJDi0B",
        "colab_type": "text"
      },
      "source": [
        "### Generating Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjdeIHR5vclZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "outputId": "e1ad4088-12e8-47e4-f95d-da48314028af"
      },
      "source": [
        "\n",
        "\n",
        "def generate_features(stances,dataset,name):\n",
        "    h, b, y = [],[],[]\n",
        "\n",
        "    for stance in stances:\n",
        "        y.append(LABELS.index(stance['Stance']))\n",
        "        h.append(stance['Headline'])\n",
        "        b.append(dataset.articles[stance['Body ID']])\n",
        "\n",
        "    X_overlap = gen_or_load_feats(word_overlap_features, h, b, \"overlap.\"+name+\".npy\")\n",
        "    X_refuting = gen_or_load_feats(refuting_features, h, b, \"refuting.\"+name+\".npy\")\n",
        "    X_polarity = gen_or_load_feats(polarity_features, h, b, \"polarity.\"+name+\".npy\")\n",
        "    X_hand = gen_or_load_feats(hand_features, h, b, \"hand.\"+name+\".npy\")\n",
        "    X_cosine=gen_or_load_feats(cosine_similarity_h,h,b,\"cosine.\"+name+\".npy\")\n",
        "\n",
        "    X = np.c_[X_hand, X_polarity, X_refuting, X_overlap,X_cosine]\n",
        "    X = preprocessing.scale(X)\n",
        "    return X,y\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "#Load the training dataset and generate folds\n",
        "d = DataSet()\n",
        "folds,hold_out = kfold_split(d,n_folds=10)\n",
        "fold_stances, hold_out_stances = get_stances_for_folds(d,folds,hold_out)\n",
        "\n",
        "# Load the competition dataset\n",
        "competition_dataset = DataSet(\"competition_test\")\n",
        "X_competition, y_competition = generate_features(competition_dataset.stances, competition_dataset, \"competition\")\n",
        "    \n",
        "  \n",
        "Xs = dict()\n",
        "ys = dict()\n",
        "\n",
        "# Load/Precompute all features now\n",
        "X_holdout,y_holdout = generate_features(hold_out_stances,d,\"holdout\")\n",
        "for fold in fold_stances:\n",
        "    Xs[fold],ys[fold] = generate_features(fold_stances[fold],d,str(fold))\n",
        "\n",
        "\n",
        "best_score = 0\n",
        "best_fold = None\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "# print(predicted)\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading dataset\n",
            "Total stances: 49972\n",
            "Total bodies: 1683\n",
            "Reading dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total stances: 25413\n",
            "Total bodies: 904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "25413it [01:28, 287.48it/s]\n",
            "25413it [00:05, 4983.83it/s]\n",
            "25413it [01:27, 289.48it/s]\n",
            "25413it [01:33, 273.24it/s]\n",
            "9622it [00:34, 276.91it/s]\n",
            "9622it [00:01, 5044.07it/s]\n",
            "9622it [00:34, 275.82it/s]\n",
            "9622it [00:38, 252.33it/s]\n",
            "4124it [00:14, 275.27it/s]\n",
            "4124it [00:00, 5178.30it/s]\n",
            "4124it [00:15, 271.80it/s]\n",
            "4124it [00:16, 253.90it/s]\n",
            "4663it [00:17, 268.28it/s]\n",
            "4663it [00:00, 5222.57it/s]\n",
            "4663it [00:17, 266.55it/s]\n",
            "4663it [00:18, 247.28it/s]\n",
            "3783it [00:11, 315.85it/s]\n",
            "3783it [00:00, 5128.81it/s]\n",
            "3783it [00:12, 311.53it/s]\n",
            "3783it [00:13, 289.90it/s]\n",
            "3388it [00:12, 262.67it/s]\n",
            "3388it [00:00, 5046.68it/s]\n",
            "3388it [00:12, 261.59it/s]\n",
            "3388it [00:13, 244.38it/s]\n",
            "3644it [00:13, 272.96it/s]\n",
            "3644it [00:00, 5084.44it/s]\n",
            "3644it [00:13, 268.19it/s]\n",
            "3644it [00:14, 254.82it/s]\n",
            "4644it [00:16, 282.02it/s]\n",
            "4644it [00:00, 5191.23it/s]\n",
            "4644it [00:16, 274.94it/s]\n",
            "4644it [00:18, 257.32it/s]\n",
            "3848it [00:14, 266.29it/s]\n",
            "3848it [00:00, 5200.73it/s]\n",
            "3848it [00:14, 259.63it/s]\n",
            "3848it [00:15, 243.55it/s]\n",
            "4273it [00:14, 296.74it/s]\n",
            "4273it [00:00, 5101.98it/s]\n",
            "4273it [00:14, 293.05it/s]\n",
            "4273it [00:15, 279.27it/s]\n",
            "4039it [00:14, 275.60it/s]\n",
            "4039it [00:00, 5190.77it/s]\n",
            "4039it [00:14, 275.17it/s]\n",
            "4039it [00:15, 253.99it/s]\n",
            "3944it [00:13, 298.86it/s]\n",
            "3944it [00:00, 5147.28it/s]\n",
            "3944it [00:13, 299.27it/s]\n",
            "3944it [00:14, 271.25it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngEaQBn2Pzcj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02b55061-c18e-4335-aa41-1a5aa00bdadc"
      },
      "source": [
        "# Classifier for each fold\n",
        "for fold in fold_stances:\n",
        "    ids = list(range(len(folds)))\n",
        "    del ids[fold]\n",
        "\n",
        "    X_train = np.vstack(tuple([Xs[i] for i in ids]))\n",
        "    y_train = np.hstack(tuple([ys[i] for i in ids]))\n",
        "\n",
        "    X_test = Xs[fold]\n",
        "    y_test = ys[fold]\n",
        "            \n",
        "            \n",
        "    gradientboosting = GradientBoostingClassifier( n_estimators=100, random_state=42,verbose=True,subsample=0.9)\n",
        "    gradientboosting.fit(X_train, y_train)\n",
        "            \n",
        "              \n",
        "    predicted = [LABELS[int(a)] for a in gradientboosting.predict(X_test)]   \n",
        "          \n",
        "            \n",
        "    actual = [LABELS[int(a)] for a in y_test]\n",
        "\n",
        "    fold_score, _ = score_submission(actual, predicted)\n",
        "    max_fold_score, _ = score_submission(actual, actual)\n",
        "\n",
        "    score = fold_score/max_fold_score\n",
        "\n",
        "    print(\"Score for fold \"+ str(fold) + \" was - \" + str(score))\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_fold = gradientboosting\n",
        "\n",
        "\n",
        "\n",
        "#Run on Holdout set and report the final score on the holdout set\n",
        "predicted = [LABELS[int(a)] for a in best_fold.predict(X_holdout)]\n",
        "    \n",
        "    \n",
        "actual = [LABELS[int(a)] for a in y_holdout]\n",
        "\n",
        "    \n",
        "report_score(actual,predicted)\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "#Run on competition dataset\n",
        "   \n",
        "predicted = [LABELS[int(a)] for a in best_fold.predict(X_competition)]\n",
        "actual = [LABELS[int(a)] for a in y_competition]\n",
        "    \n",
        "    \n",
        "a=report_score(actual,predicted)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1       22112.6978         418.3374           48.44s\n",
            "         2       19761.8563         258.3209           47.93s\n",
            "         3       17847.9261         197.7168           47.21s\n",
            "         4       16565.5343         148.5658           46.81s\n",
            "         5       15434.0585         118.7304           46.25s\n",
            "         6       14514.4496          98.5159           46.10s\n",
            "         7       13764.1054          80.3755           45.86s\n",
            "         8       13181.4071          66.8602           45.40s\n",
            "         9       12638.2679          54.6753           44.96s\n",
            "        10       12171.3476          45.0315           44.39s\n",
            "        20       10100.7488           9.1875           38.80s\n",
            "        30        9528.2547           0.9025           33.96s\n",
            "        40        9123.3210           1.0200           29.66s\n",
            "        50        8939.5114           0.3791           24.86s\n",
            "        60        8851.2218          -0.1665           19.86s\n",
            "        70        8719.6816          -0.0918           14.87s\n",
            "        80        8528.0342          -0.2377            9.89s\n",
            "        90        8451.5703          -0.0658            4.93s\n",
            "       100        8383.2512          -0.6482            0.00s\n",
            "Score for fold 6 was - 0.7931628842286699\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1       21907.8507         387.6856           47.35s\n",
            "         2       19486.1227         264.5229           46.88s\n",
            "         3       17749.9767         192.6001           46.40s\n",
            "         4       16471.3620         150.3032           46.23s\n",
            "         5       15256.1170         121.3976           45.71s\n",
            "         6       14421.6182          97.2245           45.24s\n",
            "         7       13636.9358          76.2843           44.82s\n",
            "         8       13006.4916          65.1173           44.37s\n",
            "         9       12542.8427          54.2262           43.83s\n",
            "        10       12062.8899          43.4510           43.32s\n",
            "        20        9999.9952           9.9878           37.79s\n",
            "        30        9443.8052           3.0279           32.84s\n",
            "        40        9146.0964           1.8939           28.37s\n",
            "        50        8848.6311           0.2113           23.86s\n",
            "        60        8720.2125           0.3910           19.20s\n",
            "        70        8624.2154           0.1498           14.43s\n",
            "        80        8480.8895          -0.1919            9.61s\n",
            "        90        8311.5096           0.1882            4.80s\n",
            "       100        8289.9474          -0.4405            0.00s\n",
            "Score for fold 0 was - 0.823363569502329\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1       22087.9466         429.4820           47.51s\n",
            "         2       19655.5560         274.6139           47.16s\n",
            "         3       17826.9041         203.6754           46.73s\n",
            "         4       16513.7816         149.1120           46.62s\n",
            "         5       15335.4404         121.0890           46.18s\n",
            "         6       14478.5517         100.8334           45.86s\n",
            "         7       13687.6413          82.8445           45.38s\n",
            "         8       13074.3598          66.3147           45.08s\n",
            "         9       12456.9256          54.2091           44.48s\n",
            "        10       12075.8187          47.4413           44.10s\n",
            "        20       10041.0668          11.2629           39.35s\n",
            "        30        9362.7666           3.3071           34.43s\n",
            "        40        9008.8747           2.0399           30.00s\n",
            "        50        8790.9266           0.1303           25.02s\n",
            "        60        8608.2465          -0.0753           20.05s\n",
            "        70        8541.6624          -0.0401           15.05s\n",
            "        80        8405.5068          -0.1064           10.03s\n",
            "        90        8312.0251          -0.2333            5.02s\n",
            "       100        8191.0751          -0.3538            0.00s\n",
            "Score for fold 7 was - 0.8205345778532033\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1       22024.7016         435.9900           49.81s\n",
            "         2       19616.7155         273.4393           48.30s\n",
            "         3       17820.4030         195.9706           47.37s\n",
            "         4       16441.4049         153.2159           47.16s\n",
            "         5       15365.7632         122.1619           47.03s\n",
            "         6       14411.4635          98.4036           46.66s\n",
            "         7       13622.5881          81.1334           46.23s\n",
            "         8       13050.0778          67.2591           45.73s\n",
            "         9       12543.3117          57.0805           45.19s\n",
            "        10       12093.9166          44.8627           44.65s\n",
            "        20        9988.3008           9.8955           40.02s\n",
            "        30        9355.3833           3.1400           34.79s\n",
            "        40        9031.2763          -0.1030           29.95s\n",
            "        50        8898.0667           0.8792           25.16s\n",
            "        60        8615.6085          -0.7574           20.12s\n",
            "        70        8497.9538          -0.4763           15.05s\n",
            "        80        8392.6867          -0.3529           10.05s\n",
            "        90        8258.8433          -0.4981            5.02s\n",
            "       100        8126.9132          -0.4113            0.00s\n",
            "Score for fold 5 was - 0.7839469808541973\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1       22194.4070         420.7086           48.19s\n",
            "         2       19779.9174         270.9048           48.01s\n",
            "         3       17959.4557         196.3487           47.53s\n",
            "         4       16603.5056         149.2257           47.15s\n",
            "         5       15511.8594         122.2288           46.74s\n",
            "         6       14602.8131          99.1472           46.49s\n",
            "         7       13910.4883          81.3631           46.08s\n",
            "         8       13208.1117          65.6573           45.87s\n",
            "         9       12633.8252          56.5294           45.54s\n",
            "        10       12271.9072          46.8012           44.94s\n",
            "        20       10183.7280          10.0580           39.62s\n",
            "        30        9568.5745           4.0507           34.98s\n",
            "        40        9206.9733           0.9918           30.01s\n",
            "        50        9001.6804           0.3452           25.08s\n",
            "        60        8825.1934          -0.2462           20.05s\n",
            "        70        8716.2480          -0.3025           15.05s\n",
            "        80        8578.4964          -0.1536           10.01s\n",
            "        90        8487.2534          -0.4463            5.00s\n",
            "       100        8403.3948          -0.1398            0.00s\n",
            "Score for fold 2 was - 0.8343995347484734\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1       21877.2152         430.2715           47.66s\n",
            "         2       19581.2765         259.8924           46.43s\n",
            "         3       17855.7528         188.8735           45.52s\n",
            "         4       16445.0890         153.1777           45.19s\n",
            "         5       15324.4535         118.3645           44.52s\n",
            "         6       14490.9857          96.1311           44.04s\n",
            "         7       13790.2763          77.0378           43.52s\n",
            "         8       13082.2797          66.4789           43.26s\n",
            "         9       12594.0354          54.6846           42.73s\n",
            "        10       12078.7999          44.4436           42.20s\n",
            "        20       10053.9506          10.8230           37.88s\n",
            "        30        9447.1983           4.0390           33.67s\n",
            "        40        9186.3520           1.7473           28.78s\n",
            "        50        8950.0237           1.2061           23.97s\n",
            "        60        8754.3278          -0.2502           19.11s\n",
            "        70        8702.2133          -0.0074           14.34s\n",
            "        80        8491.8404          -0.3801            9.53s\n",
            "        90        8412.7301          -0.3573            4.76s\n",
            "       100        8323.0400           0.0092            0.00s\n",
            "Score for fold 8 was - 0.8425396825396826\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1       21973.4086         434.0219           46.61s\n",
            "         2       19694.0029         264.8243           46.79s\n",
            "         3       17838.3648         191.1895           46.41s\n",
            "         4       16486.0838         151.6052           45.78s\n",
            "         5       15407.9483         118.5553           45.21s\n",
            "         6       14545.4974         100.0873           45.19s\n",
            "         7       13731.1083          81.2459           44.86s\n",
            "         8       13123.8798          67.6364           44.83s\n",
            "         9       12607.8895          54.1862           44.24s\n",
            "        10       12182.5706          43.8482           43.71s\n",
            "        20       10144.7550          10.1861           38.73s\n",
            "        30        9488.2533           2.7972           33.83s\n",
            "        40        9105.9858           0.4399           29.08s\n",
            "        50        8911.6176           0.6028           24.40s\n",
            "        60        8722.5540           0.2038           19.68s\n",
            "        70        8599.2978          -0.1930           14.91s\n",
            "        80        8575.2948          -0.0408           10.00s\n",
            "        90        8470.4018          -0.1846            4.98s\n",
            "       100        8328.6415           0.0799            0.00s\n",
            "Score for fold 9 was - 0.8100753558470556\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1       22115.8467         416.1453           49.15s\n",
            "         2       19665.0562         268.8175           48.56s\n",
            "         3       17846.2419         196.6135           47.63s\n",
            "         4       16473.4908         153.8128           46.69s\n",
            "         5       15383.7101         122.6526           46.37s\n",
            "         6       14544.3625          97.0031           45.74s\n",
            "         7       13785.5610          77.5107           45.11s\n",
            "         8       13135.9909          66.0651           44.48s\n",
            "         9       12625.8900          55.8632           44.01s\n",
            "        10       12198.2399          44.0886           43.69s\n",
            "        20       10074.9556           9.8681           38.61s\n",
            "        30        9503.4420           3.1339           33.95s\n",
            "        40        9140.9600           0.7306           29.14s\n",
            "        50        8975.0367           0.2370           24.41s\n",
            "        60        8757.4380           0.4191           19.54s\n",
            "        70        8595.3710          -0.1819           14.67s\n",
            "        80        8505.6201          -0.3710            9.76s\n",
            "        90        8403.2296           0.2546            4.88s\n",
            "       100        8317.0631          -0.1681            0.00s\n",
            "Score for fold 3 was - 0.8316016731885035\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1       22128.7404         413.8039           48.88s\n",
            "         2       19748.2394         258.9479           47.28s\n",
            "         3       17971.2872         194.7721           46.78s\n",
            "         4       16557.9015         146.3468           46.76s\n",
            "         5       15482.4250         118.6763           46.44s\n",
            "         6       14551.4431          97.9772           46.24s\n",
            "         7       13847.4326          78.2443           46.04s\n",
            "         8       13250.7605          66.1878           45.58s\n",
            "         9       12739.1731          53.7869           45.38s\n",
            "        10       12255.5798          46.0776           44.68s\n",
            "        20       10117.9210           6.6361           39.56s\n",
            "        30        9560.0570           2.1567           34.65s\n",
            "        40        9292.9587          -0.0756           29.53s\n",
            "        50        8996.6686           0.6605           24.58s\n",
            "        60        8889.5450          -0.5216           19.72s\n",
            "        70        8717.2627          -0.4980           14.90s\n",
            "        80        8555.7063           0.0804            9.96s\n",
            "        90        8503.6500          -0.1063            5.01s\n",
            "       100        8315.2239          -0.5902            0.00s\n",
            "Score for fold 1 was - 0.8372677748288867\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1       22152.7241         427.0081           50.90s\n",
            "         2       19758.3108         270.6787           49.95s\n",
            "         3       18015.9110         191.3636           48.60s\n",
            "         4       16573.1719         158.3212           47.73s\n",
            "         5       15429.4070         119.4489           47.03s\n",
            "         6       14617.8386         101.2118           46.16s\n",
            "         7       13869.7411          80.3320           45.54s\n",
            "         8       13305.5555          66.9928           45.13s\n",
            "         9       12716.9144          55.5344           44.44s\n",
            "        10       12233.0154          43.9143           44.05s\n",
            "        20       10130.8536           9.2910           38.37s\n",
            "        30        9550.5468           3.0372           33.68s\n",
            "        40        9235.2441           0.7122           29.11s\n",
            "        50        8980.5172           0.3534           24.49s\n",
            "        60        8841.6242          -0.0296           19.64s\n",
            "        70        8693.8740           0.4904           14.74s\n",
            "        80        8560.8972          -0.1238            9.82s\n",
            "        90        8493.5374          -0.2150            4.92s\n",
            "       100        8385.6941          -0.5415            0.00s\n",
            "Score for fold 4 was - 0.809700958649306\n",
            "-------------------------------------------------------------\n",
            "|           |   agree   | disagree  |  discuss  | unrelated |\n",
            "-------------------------------------------------------------\n",
            "|   agree   |    101    |     2     |    612    |    47     |\n",
            "-------------------------------------------------------------\n",
            "| disagree  |    15     |     0     |    141    |     6     |\n",
            "-------------------------------------------------------------\n",
            "|  discuss  |    57     |     0     |   1623    |    120    |\n",
            "-------------------------------------------------------------\n",
            "| unrelated |     3     |     0     |    75     |   6820    |\n",
            "-------------------------------------------------------------\n",
            "Score: 3635.75 out of 4448.5\t(81.72979656063842%)\n",
            "\n",
            "\n",
            "-------------------------------------------------------------\n",
            "|           |   agree   | disagree  |  discuss  | unrelated |\n",
            "-------------------------------------------------------------\n",
            "|   agree   |    163    |     1     |   1542    |    197    |\n",
            "-------------------------------------------------------------\n",
            "| disagree  |    43     |     1     |    478    |    175    |\n",
            "-------------------------------------------------------------\n",
            "|  discuss  |    173    |     4     |   3830    |    457    |\n",
            "-------------------------------------------------------------\n",
            "| unrelated |     8     |     1     |    306    |   18034   |\n",
            "-------------------------------------------------------------\n",
            "Score: 9062.75 out of 11651.25\t(77.7834996245038%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZRdgLDIn_ZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data = {'Iterations':  [1, 2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100],\n",
        "        'Loss': [22194.4070,19779.9174,17959.4557,16603.5056,15511.8594, 14602.8131,13910.4883,13208.1117,12633.8252,12271.9072,10183.7280,9568.5745,9206.9733,9001.6804,8825.1934,\n",
        "                8716.2480, 8578.4964,8487.2534,8403.3948],\n",
        "        \n",
        "        }\n",
        "gb = pd.DataFrame (data, columns = ['Iterations','Loss'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAQYg3Iepj_2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "14307c39-0724-4f35-a368-86bd623a2930"
      },
      "source": [
        "# Plotting train loss vs Iterations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(gb['Loss'])\n",
        "plt.xlabel('Iterations')\n",
        "plt.xticks([1,3,5,7,9,11,13,15,17,20])\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train Loss')\n",
        "plt.legend(['Training Loss'])\n",
        "plt.savefig('Gradient_loss.png')\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVSj0M3ms5dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comp_test=pd.read_csv('/content/drive/My Drive/fnc-1/competition_test_stances.csv')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLlbonFrsRnp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e435ef09-bd27-4a5a-badb-bb0e626ad0a5"
      },
      "source": [
        "#General Loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(comp_test['Stance'],predicted)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8668004564592925"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHhexcLk6p3v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "38fd5046-f860-4a93-a2c6-77d0549fa6dc"
      },
      "source": [
        "#Generating answers.csv\n",
        "body_id = []\n",
        "headlines = []\n",
        "stances = np.copy(np.array(predicted))\n",
        "\n",
        "for i in range(len(competition_dataset.stances)):\n",
        "    body_id.append(competition_dataset.stances[i]['Body ID'])\n",
        "    headlines.append(competition_dataset.stances[i]['Headline'])\n",
        "test_data = pd.DataFrame(columns=['Headline', 'Body ID', 'Stance'])\n",
        "test_data['Headline'] = headlines\n",
        "test_data['Body ID'] = body_id\n",
        "test_data['Stance'] = stances\n",
        "print(test_data.head())\n",
        "\n",
        "test_data.to_csv('Gradient_Boosting_cosine_sim_tuned.csv', index=False, encoding='utf-8')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            Headline  Body ID     Stance\n",
            "0  Ferguson riots: Pregnant woman loses eye after...     2008  unrelated\n",
            "1  Crazy Conservatives Are Sure a Gitmo Detainee ...     1550  unrelated\n",
            "2  A Russian Guy Says His Justin Bieber Ringtone ...        2  unrelated\n",
            "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...     1793  unrelated\n",
            "4  Argentina's President Adopts Boy to End Werewo...       37  unrelated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFB-fXWMQqwn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "a12bc494-5131-4a39-977f-8a594b69653b"
      },
      "source": [
        "#Calculating f1 score of individual stances type(Competition set)\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_f1scores(y_true, y_predicted):\n",
        "    \n",
        "    f1_macro = f1_score(y_true, y_predicted, average='macro')\n",
        "    f1_classwise = f1_score(y_true, y_predicted, average=None, labels=[\"agree\", \"disagree\", \"discuss\", \"unrelated\"])\n",
        "\n",
        "    result = \"F1 macro: {:.3f}\".format(f1_macro * 100) + \"% \\n\"\n",
        "    result += \"F1 agree: {:.3f}\".format(f1_classwise[0] * 100) + \"% \\n\"\n",
        "    result += \"F1 disagree: {:.3f}\".format(f1_classwise[1] * 100) + \"% \\n\"\n",
        "    result += \"F1 discuss: {:.3f}\".format(f1_classwise[2] * 100) + \"% \\n\"\n",
        "    result += \"F1 unrelated: {:.3f}\".format(f1_classwise[3] * 100) + \"% \\n\"\n",
        "    return result\n",
        "print(calculate_f1scores(predicted,actual))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 macro: 45.893% \n",
            "F1 agree: 14.236% \n",
            "F1 disagree: 0.284% \n",
            "F1 discuss: 72.128% \n",
            "F1 unrelated: 96.926% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmQT6ieurblj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "db188ead-48bb-4ee9-f7b0-2d7a2d7f96c1"
      },
      "source": [
        "# calculating precision,recall,f1-score,support for Competition set\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "eval_report = classification_report(predicted,actual)\n",
        "print('Test report', eval_report)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test report               precision    recall  f1-score   support\n",
            "\n",
            "       agree       0.09      0.42      0.14       387\n",
            "    disagree       0.00      0.14      0.00         7\n",
            "     discuss       0.86      0.62      0.72      6156\n",
            "   unrelated       0.98      0.96      0.97     18863\n",
            "\n",
            "    accuracy                           0.87     25413\n",
            "   macro avg       0.48      0.54      0.46     25413\n",
            "weighted avg       0.94      0.87      0.90     25413\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}